{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from gradient_2d import numerical_gradient\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예 : lr = 10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "gradient_descent(function_2, init_x=init_x, lr=10, step_num=100)\n",
    "# array([-2.58983747e+13, -1.29524862e+12])\n",
    "\n",
    "# 학습률이 너무 큰 예 : lr = 1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)\n",
    "# array([-2.99999994,  3.99999992])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06962953  0.33930864 -0.40893817]\n",
      " [ 0.10444429  0.50896296 -0.61340725]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59621865  0.42721305 -1.69790677]\n",
      " [ 1.63149782  0.45348609  1.63496106]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.82607922 0.66446531 0.45272089]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8220327888714216"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38308388  0.11989784 -0.50298172]\n",
      " [ 0.57462581  0.17984676 -0.75447257]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
